{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Audio Emotion Recognition</center>\n",
    "## <center>Part 3 - Baseline model</center>\n",
    "#### <center> 24th August 2019 </center> \n",
    "#####  <center> Eu Jin Lok </center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "Continuing where we left off in [Part 1](https://www.kaggle.com/ejlok1/audio-emotion-recognition-part-1-explore-data) and [Part 2](https://www.kaggle.com/ejlok1/audio-emotion-recognition-part-2-feature-extra), here we'll build a baseline model for an emotion classifier. When I say baseline, I mean its the simplest most parsimonious model I can think of. And view points will vary from one data scientist to another, but essentially its a model __NOT__ meant to achieve full accuracy potential. It's just to qucikly test the framework and setup the blueprint for how we go about creating a workable emotion classifier, cause at the moment, we don't know what works and what doesn't. This is a long notebook so this is the agenda below: \n",
    "\n",
    "1. [Data preparation and processing](#data)\n",
    "    * [Data preparation](#preparation)\n",
    "    * [Data processing](#processing)\n",
    "2. [Modelling](#modelling)\n",
    "3. [Model serialisation](#serialise)\n",
    "4. [Model validation](#validation)\n",
    "5. [Final thoughts](#final)\n",
    "\n",
    "Upvote this notebook if you like, and be sure to check out the other parts which are now available:\n",
    "* [Part 4 | Apply to new audio data](https://www.kaggle.com/ejlok1/audio-emotion-part-4-apply-to-new-audio-data)\n",
    "* [Part 5 | Data augmentation](https://www.kaggle.com/ejlok1/audio-emotion-part-5-data-augmentation)\n",
    "\n",
    "Most importantly, I want to thank the 4 authors for their excellent dataset, without it, writing this notebook could not have been possible. The original source of the dataset links are below:\n",
    "\n",
    "- [TESS](https://tspace.library.utoronto.ca/handle/1807/24487)\n",
    "- [CREMA-D](https://github.com/CheyneyComputerScience/CREMA-D)\n",
    "- [SAVEE](http://kahlan.eps.surrey.ac.uk/savee/Database.html)\n",
    "- [RAVDESS](https://zenodo.org/record/1188976#.XYP8CSgzaUk)\n",
    "- [RAVDESS_Kaggle](https://www.kaggle.com/uwrfkaggler/ravdess-emotional-speech-audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# Importing required libraries \n",
    "# Keras\n",
    "import keras\n",
    "from keras import regularizers\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, Model, model_from_json\n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "from keras.layers import Input, Flatten, Dropout, Activation, BatchNormalization\n",
    "from keras.layers import Conv1D, MaxPooling1D, AveragePooling1D\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# sklearn\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Other  \n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import librosa.display\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from matplotlib.pyplot import specgram\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import glob \n",
    "import os\n",
    "import pickle\n",
    "import IPython.display as ipd  # To play sound in the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"data\"></a>\n",
    "## 1. Data preparation and processing\n",
    "We saw in [Part 1](https://www.kaggle.com/ejlok1/audio-emotion-recognition-part-1-explore-data) and [Part 2](https://www.kaggle.com/ejlok1/audio-emotion-recognition-part-2-feature-extra) the way we process the audio file into data and the MFCC features we extracted. We're going to do the same thing here except we process the entirity of the audio files. First up we need the reference file that contains the path to the raw audio files for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"preparation\"></a>\n",
    "### Data preparation\n",
    "Lets pick up the meta-data file which we save in [part 1](\"https://www.kaggle.com/ejlok1/audio-emotion-recognition-part-1-explore-data\"), we're going to need it here to run a loop over it to read all the audio files spread across the 4 directories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>source</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>male_sad</td>\n",
       "      <td>SAVEE</td>\n",
       "      <td>input/ALL/JK_sa01.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>male_sad</td>\n",
       "      <td>SAVEE</td>\n",
       "      <td>input/ALL/JK_sa15.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>male_neutral</td>\n",
       "      <td>SAVEE</td>\n",
       "      <td>input/ALL/DC_n13.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>male_surprise</td>\n",
       "      <td>SAVEE</td>\n",
       "      <td>input/ALL/DC_su09.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>male_neutral</td>\n",
       "      <td>SAVEE</td>\n",
       "      <td>input/ALL/DC_n07.wav</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          labels source                   path\n",
       "0       male_sad  SAVEE  input/ALL/JK_sa01.wav\n",
       "1       male_sad  SAVEE  input/ALL/JK_sa15.wav\n",
       "2   male_neutral  SAVEE   input/ALL/DC_n13.wav\n",
       "3  male_surprise  SAVEE  input/ALL/DC_su09.wav\n",
       "4   male_neutral  SAVEE   input/ALL/DC_n07.wav"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets pick up the meta-data that we got from our first part of the Kernel\n",
    "ref = pd.read_csv(\"Data_path.csv\")\n",
    "ref.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we've already seen the shape of an MFCC output for each file, and it's a 2D matrix of the number of bands by time. In order to optimise space and memory, we're going to read each audio file, extract its mean across all MFCC bands by time, and  just keep the extracted features, dropping the entire audio file data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12162\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-8.812322, -12.111704, -22.594236, -21.481213...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-8.84645, -13.126471, -24.391258, -23.972637,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-2.0016851, -2.2841876, -8.079337, -7.4936037...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-3.6122117, -4.3207064, -7.5397215, -8.864785...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-0.8923286, -2.7065654, -9.859099, -8.692252,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             feature\n",
       "0  [-8.812322, -12.111704, -22.594236, -21.481213...\n",
       "1  [-8.84645, -13.126471, -24.391258, -23.972637,...\n",
       "2  [-2.0016851, -2.2841876, -8.079337, -7.4936037...\n",
       "3  [-3.6122117, -4.3207064, -7.5397215, -8.864785...\n",
       "4  [-0.8923286, -2.7065654, -9.859099, -8.692252,..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note this takes a couple of minutes (~10 mins) as we're iterating over 4 datasets \n",
    "df = pd.DataFrame(columns=['feature'])\n",
    "\n",
    "# loop feature extraction over the entire dataset\n",
    "counter=0\n",
    "for index,path in enumerate(ref.path):\n",
    "    X, sample_rate = librosa.load(path\n",
    "                                  , res_type='kaiser_fast'\n",
    "                                  ,duration=2.5\n",
    "                                  ,sr=44100\n",
    "                                  ,offset=0.5\n",
    "                                 )\n",
    "    sample_rate = np.array(sample_rate)\n",
    "    \n",
    "    # mean as the feature. Could do min and max etc as well. \n",
    "    mfccs = np.mean(librosa.feature.mfcc(y=X, \n",
    "                                        sr=sample_rate, \n",
    "                                        n_mfcc=13),\n",
    "                    axis=0)\n",
    "    df.loc[counter] = [mfccs]\n",
    "    counter=counter+1   \n",
    "\n",
    "# Check a few records to make sure its processed successfully\n",
    "print(len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"processing\"></a>\n",
    "### Data processing\n",
    "\n",
    "Like any good standard data science workflow, data processing is the most important step. Cause garbage in grabage out. So lets start munging the data into a workable format and pad out any issues we find. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>source</th>\n",
       "      <th>path</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>...</th>\n",
       "      <th>206</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "      <th>213</th>\n",
       "      <th>214</th>\n",
       "      <th>215</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>male_sad</td>\n",
       "      <td>SAVEE</td>\n",
       "      <td>input/ALL/JK_sa01.wav</td>\n",
       "      <td>-8.812322</td>\n",
       "      <td>-12.111704</td>\n",
       "      <td>-22.594236</td>\n",
       "      <td>-21.481213</td>\n",
       "      <td>-20.949923</td>\n",
       "      <td>-20.414585</td>\n",
       "      <td>-20.267546</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.499665</td>\n",
       "      <td>-10.080903</td>\n",
       "      <td>-12.700766</td>\n",
       "      <td>-17.040064</td>\n",
       "      <td>-20.240370</td>\n",
       "      <td>-23.302591</td>\n",
       "      <td>-24.621035</td>\n",
       "      <td>-23.829395</td>\n",
       "      <td>-14.862103</td>\n",
       "      <td>-9.119078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>male_sad</td>\n",
       "      <td>SAVEE</td>\n",
       "      <td>input/ALL/JK_sa15.wav</td>\n",
       "      <td>-8.846450</td>\n",
       "      <td>-13.126471</td>\n",
       "      <td>-24.391258</td>\n",
       "      <td>-23.972637</td>\n",
       "      <td>-23.494141</td>\n",
       "      <td>-24.208843</td>\n",
       "      <td>-25.631187</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.738687</td>\n",
       "      <td>-8.822191</td>\n",
       "      <td>-8.977811</td>\n",
       "      <td>-9.998902</td>\n",
       "      <td>-15.777987</td>\n",
       "      <td>-22.670012</td>\n",
       "      <td>-23.585888</td>\n",
       "      <td>-24.138813</td>\n",
       "      <td>-17.324165</td>\n",
       "      <td>-9.067061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>male_neutral</td>\n",
       "      <td>SAVEE</td>\n",
       "      <td>input/ALL/DC_n13.wav</td>\n",
       "      <td>-2.001685</td>\n",
       "      <td>-2.284188</td>\n",
       "      <td>-8.079337</td>\n",
       "      <td>-7.493604</td>\n",
       "      <td>-7.611508</td>\n",
       "      <td>-5.591492</td>\n",
       "      <td>-4.388683</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>male_surprise</td>\n",
       "      <td>SAVEE</td>\n",
       "      <td>input/ALL/DC_su09.wav</td>\n",
       "      <td>-3.612212</td>\n",
       "      <td>-4.320706</td>\n",
       "      <td>-7.539721</td>\n",
       "      <td>-8.864785</td>\n",
       "      <td>-8.661814</td>\n",
       "      <td>-8.826547</td>\n",
       "      <td>-9.143904</td>\n",
       "      <td>...</td>\n",
       "      <td>-25.902903</td>\n",
       "      <td>-25.006645</td>\n",
       "      <td>-24.709747</td>\n",
       "      <td>-25.516712</td>\n",
       "      <td>-26.941380</td>\n",
       "      <td>-25.354641</td>\n",
       "      <td>-25.213074</td>\n",
       "      <td>-27.607460</td>\n",
       "      <td>-16.149429</td>\n",
       "      <td>-8.528479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>male_neutral</td>\n",
       "      <td>SAVEE</td>\n",
       "      <td>input/ALL/DC_n07.wav</td>\n",
       "      <td>-0.892329</td>\n",
       "      <td>-2.706565</td>\n",
       "      <td>-9.859099</td>\n",
       "      <td>-8.692252</td>\n",
       "      <td>-8.685309</td>\n",
       "      <td>-8.844451</td>\n",
       "      <td>-8.032233</td>\n",
       "      <td>...</td>\n",
       "      <td>-11.438392</td>\n",
       "      <td>-11.691319</td>\n",
       "      <td>-11.480921</td>\n",
       "      <td>-10.730119</td>\n",
       "      <td>-9.891496</td>\n",
       "      <td>-9.329518</td>\n",
       "      <td>-8.907435</td>\n",
       "      <td>-8.881424</td>\n",
       "      <td>-8.354049</td>\n",
       "      <td>-5.121763</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 219 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          labels source                   path         0          1  \\\n",
       "0       male_sad  SAVEE  input/ALL/JK_sa01.wav -8.812322 -12.111704   \n",
       "1       male_sad  SAVEE  input/ALL/JK_sa15.wav -8.846450 -13.126471   \n",
       "2   male_neutral  SAVEE   input/ALL/DC_n13.wav -2.001685  -2.284188   \n",
       "3  male_surprise  SAVEE  input/ALL/DC_su09.wav -3.612212  -4.320706   \n",
       "4   male_neutral  SAVEE   input/ALL/DC_n07.wav -0.892329  -2.706565   \n",
       "\n",
       "           2          3          4          5          6  ...        206  \\\n",
       "0 -22.594236 -21.481213 -20.949923 -20.414585 -20.267546  ...  -8.499665   \n",
       "1 -24.391258 -23.972637 -23.494141 -24.208843 -25.631187  ...  -8.738687   \n",
       "2  -8.079337  -7.493604  -7.611508  -5.591492  -4.388683  ...        NaN   \n",
       "3  -7.539721  -8.864785  -8.661814  -8.826547  -9.143904  ... -25.902903   \n",
       "4  -9.859099  -8.692252  -8.685309  -8.844451  -8.032233  ... -11.438392   \n",
       "\n",
       "         207        208        209        210        211        212  \\\n",
       "0 -10.080903 -12.700766 -17.040064 -20.240370 -23.302591 -24.621035   \n",
       "1  -8.822191  -8.977811  -9.998902 -15.777987 -22.670012 -23.585888   \n",
       "2        NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "3 -25.006645 -24.709747 -25.516712 -26.941380 -25.354641 -25.213074   \n",
       "4 -11.691319 -11.480921 -10.730119  -9.891496  -9.329518  -8.907435   \n",
       "\n",
       "         213        214       215  \n",
       "0 -23.829395 -14.862103 -9.119078  \n",
       "1 -24.138813 -17.324165 -9.067061  \n",
       "2        NaN        NaN       NaN  \n",
       "3 -27.607460 -16.149429 -8.528479  \n",
       "4  -8.881424  -8.354049 -5.121763  \n",
       "\n",
       "[5 rows x 219 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now extract the mean bands to its own feature columns\n",
    "df = pd.concat([ref,pd.DataFrame(df['feature'].values.tolist())],axis=1)\n",
    "df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12162, 219)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>source</th>\n",
       "      <th>path</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>...</th>\n",
       "      <th>206</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "      <th>213</th>\n",
       "      <th>214</th>\n",
       "      <th>215</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>male_sad</td>\n",
       "      <td>SAVEE</td>\n",
       "      <td>input/ALL/JK_sa01.wav</td>\n",
       "      <td>-8.812322</td>\n",
       "      <td>-12.111704</td>\n",
       "      <td>-22.594236</td>\n",
       "      <td>-21.481213</td>\n",
       "      <td>-20.949923</td>\n",
       "      <td>-20.414585</td>\n",
       "      <td>-20.267546</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.499665</td>\n",
       "      <td>-10.080903</td>\n",
       "      <td>-12.700766</td>\n",
       "      <td>-17.040064</td>\n",
       "      <td>-20.240370</td>\n",
       "      <td>-23.302591</td>\n",
       "      <td>-24.621035</td>\n",
       "      <td>-23.829395</td>\n",
       "      <td>-14.862103</td>\n",
       "      <td>-9.119078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>male_sad</td>\n",
       "      <td>SAVEE</td>\n",
       "      <td>input/ALL/JK_sa15.wav</td>\n",
       "      <td>-8.846450</td>\n",
       "      <td>-13.126471</td>\n",
       "      <td>-24.391258</td>\n",
       "      <td>-23.972637</td>\n",
       "      <td>-23.494141</td>\n",
       "      <td>-24.208843</td>\n",
       "      <td>-25.631187</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.738687</td>\n",
       "      <td>-8.822191</td>\n",
       "      <td>-8.977811</td>\n",
       "      <td>-9.998902</td>\n",
       "      <td>-15.777987</td>\n",
       "      <td>-22.670012</td>\n",
       "      <td>-23.585888</td>\n",
       "      <td>-24.138813</td>\n",
       "      <td>-17.324165</td>\n",
       "      <td>-9.067061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>male_neutral</td>\n",
       "      <td>SAVEE</td>\n",
       "      <td>input/ALL/DC_n13.wav</td>\n",
       "      <td>-2.001685</td>\n",
       "      <td>-2.284188</td>\n",
       "      <td>-8.079337</td>\n",
       "      <td>-7.493604</td>\n",
       "      <td>-7.611508</td>\n",
       "      <td>-5.591492</td>\n",
       "      <td>-4.388683</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>male_surprise</td>\n",
       "      <td>SAVEE</td>\n",
       "      <td>input/ALL/DC_su09.wav</td>\n",
       "      <td>-3.612212</td>\n",
       "      <td>-4.320706</td>\n",
       "      <td>-7.539721</td>\n",
       "      <td>-8.864785</td>\n",
       "      <td>-8.661814</td>\n",
       "      <td>-8.826547</td>\n",
       "      <td>-9.143904</td>\n",
       "      <td>...</td>\n",
       "      <td>-25.902903</td>\n",
       "      <td>-25.006645</td>\n",
       "      <td>-24.709747</td>\n",
       "      <td>-25.516712</td>\n",
       "      <td>-26.941380</td>\n",
       "      <td>-25.354641</td>\n",
       "      <td>-25.213074</td>\n",
       "      <td>-27.607460</td>\n",
       "      <td>-16.149429</td>\n",
       "      <td>-8.528479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>male_neutral</td>\n",
       "      <td>SAVEE</td>\n",
       "      <td>input/ALL/DC_n07.wav</td>\n",
       "      <td>-0.892329</td>\n",
       "      <td>-2.706565</td>\n",
       "      <td>-9.859099</td>\n",
       "      <td>-8.692252</td>\n",
       "      <td>-8.685309</td>\n",
       "      <td>-8.844451</td>\n",
       "      <td>-8.032233</td>\n",
       "      <td>...</td>\n",
       "      <td>-11.438392</td>\n",
       "      <td>-11.691319</td>\n",
       "      <td>-11.480921</td>\n",
       "      <td>-10.730119</td>\n",
       "      <td>-9.891496</td>\n",
       "      <td>-9.329518</td>\n",
       "      <td>-8.907435</td>\n",
       "      <td>-8.881424</td>\n",
       "      <td>-8.354049</td>\n",
       "      <td>-5.121763</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 219 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          labels source                   path         0          1  \\\n",
       "0       male_sad  SAVEE  input/ALL/JK_sa01.wav -8.812322 -12.111704   \n",
       "1       male_sad  SAVEE  input/ALL/JK_sa15.wav -8.846450 -13.126471   \n",
       "2   male_neutral  SAVEE   input/ALL/DC_n13.wav -2.001685  -2.284188   \n",
       "3  male_surprise  SAVEE  input/ALL/DC_su09.wav -3.612212  -4.320706   \n",
       "4   male_neutral  SAVEE   input/ALL/DC_n07.wav -0.892329  -2.706565   \n",
       "\n",
       "           2          3          4          5          6  ...        206  \\\n",
       "0 -22.594236 -21.481213 -20.949923 -20.414585 -20.267546  ...  -8.499665   \n",
       "1 -24.391258 -23.972637 -23.494141 -24.208843 -25.631187  ...  -8.738687   \n",
       "2  -8.079337  -7.493604  -7.611508  -5.591492  -4.388683  ...   0.000000   \n",
       "3  -7.539721  -8.864785  -8.661814  -8.826547  -9.143904  ... -25.902903   \n",
       "4  -9.859099  -8.692252  -8.685309  -8.844451  -8.032233  ... -11.438392   \n",
       "\n",
       "         207        208        209        210        211        212  \\\n",
       "0 -10.080903 -12.700766 -17.040064 -20.240370 -23.302591 -24.621035   \n",
       "1  -8.822191  -8.977811  -9.998902 -15.777987 -22.670012 -23.585888   \n",
       "2   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "3 -25.006645 -24.709747 -25.516712 -26.941380 -25.354641 -25.213074   \n",
       "4 -11.691319 -11.480921 -10.730119  -9.891496  -9.329518  -8.907435   \n",
       "\n",
       "         213        214       215  \n",
       "0 -23.829395 -14.862103 -9.119078  \n",
       "1 -24.138813 -17.324165 -9.067061  \n",
       "2   0.000000   0.000000  0.000000  \n",
       "3 -27.607460 -16.149429 -8.528479  \n",
       "4  -8.881424  -8.354049 -5.121763  \n",
       "\n",
       "[5 rows x 219 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# replace NA with 0\n",
    "df=df.fillna(0)\n",
    "print(df.shape)\n",
    "df[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that looks alot better. Next step we will split the data into 2 parts, one for training and one for validation. This ensures we measure the model's performance at its true accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>206</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "      <th>213</th>\n",
       "      <th>214</th>\n",
       "      <th>215</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4950</th>\n",
       "      <td>-17.142696</td>\n",
       "      <td>-17.249537</td>\n",
       "      <td>-18.365582</td>\n",
       "      <td>-18.948351</td>\n",
       "      <td>-17.365459</td>\n",
       "      <td>-16.711090</td>\n",
       "      <td>-17.699482</td>\n",
       "      <td>-18.021383</td>\n",
       "      <td>-17.897398</td>\n",
       "      <td>-15.878503</td>\n",
       "      <td>...</td>\n",
       "      <td>-22.865425</td>\n",
       "      <td>-21.614164</td>\n",
       "      <td>-19.724932</td>\n",
       "      <td>-18.845335</td>\n",
       "      <td>-19.363422</td>\n",
       "      <td>-20.137630</td>\n",
       "      <td>-22.65514</td>\n",
       "      <td>-24.578310</td>\n",
       "      <td>-24.039165</td>\n",
       "      <td>-23.209587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3860</th>\n",
       "      <td>-18.354397</td>\n",
       "      <td>-20.836168</td>\n",
       "      <td>-22.571033</td>\n",
       "      <td>-22.155430</td>\n",
       "      <td>-20.623474</td>\n",
       "      <td>-17.927940</td>\n",
       "      <td>-15.832610</td>\n",
       "      <td>-18.623466</td>\n",
       "      <td>-21.543407</td>\n",
       "      <td>-24.989613</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9761</th>\n",
       "      <td>-4.823565</td>\n",
       "      <td>-6.056048</td>\n",
       "      <td>-9.580621</td>\n",
       "      <td>-12.012063</td>\n",
       "      <td>-9.959867</td>\n",
       "      <td>-11.912548</td>\n",
       "      <td>-13.994515</td>\n",
       "      <td>-13.555813</td>\n",
       "      <td>-14.022305</td>\n",
       "      <td>-15.118246</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7620</th>\n",
       "      <td>-7.031146</td>\n",
       "      <td>-4.253551</td>\n",
       "      <td>-4.534490</td>\n",
       "      <td>-5.836689</td>\n",
       "      <td>-5.248197</td>\n",
       "      <td>-6.456452</td>\n",
       "      <td>-8.122451</td>\n",
       "      <td>-9.154640</td>\n",
       "      <td>-8.647813</td>\n",
       "      <td>-8.178625</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11586</th>\n",
       "      <td>-22.565975</td>\n",
       "      <td>-21.767015</td>\n",
       "      <td>-20.529488</td>\n",
       "      <td>-20.669310</td>\n",
       "      <td>-21.171085</td>\n",
       "      <td>-18.573399</td>\n",
       "      <td>-18.412350</td>\n",
       "      <td>-16.178038</td>\n",
       "      <td>-14.222460</td>\n",
       "      <td>-15.122540</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7914</th>\n",
       "      <td>-20.082027</td>\n",
       "      <td>-18.982424</td>\n",
       "      <td>-17.009443</td>\n",
       "      <td>-16.944057</td>\n",
       "      <td>-19.334974</td>\n",
       "      <td>-19.527683</td>\n",
       "      <td>-21.974346</td>\n",
       "      <td>-20.144060</td>\n",
       "      <td>-17.954927</td>\n",
       "      <td>-19.302570</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9513</th>\n",
       "      <td>-20.103537</td>\n",
       "      <td>-18.625866</td>\n",
       "      <td>-16.116108</td>\n",
       "      <td>-16.929594</td>\n",
       "      <td>-18.197668</td>\n",
       "      <td>-18.149664</td>\n",
       "      <td>-19.240425</td>\n",
       "      <td>-18.361637</td>\n",
       "      <td>-16.917982</td>\n",
       "      <td>-16.639193</td>\n",
       "      <td>...</td>\n",
       "      <td>-17.848896</td>\n",
       "      <td>-19.357054</td>\n",
       "      <td>-17.748692</td>\n",
       "      <td>-19.136808</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5835</th>\n",
       "      <td>-21.078182</td>\n",
       "      <td>-18.671947</td>\n",
       "      <td>-17.676802</td>\n",
       "      <td>-18.009502</td>\n",
       "      <td>-18.218781</td>\n",
       "      <td>-18.676929</td>\n",
       "      <td>-16.592762</td>\n",
       "      <td>-17.796528</td>\n",
       "      <td>-17.992470</td>\n",
       "      <td>-17.824917</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5389</th>\n",
       "      <td>-22.707445</td>\n",
       "      <td>-20.160748</td>\n",
       "      <td>-18.926155</td>\n",
       "      <td>-19.429979</td>\n",
       "      <td>-19.278114</td>\n",
       "      <td>-18.125767</td>\n",
       "      <td>-18.068565</td>\n",
       "      <td>-19.879320</td>\n",
       "      <td>-20.896755</td>\n",
       "      <td>-19.176098</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11222</th>\n",
       "      <td>-20.008287</td>\n",
       "      <td>-17.640305</td>\n",
       "      <td>-19.188612</td>\n",
       "      <td>-18.435749</td>\n",
       "      <td>-17.538836</td>\n",
       "      <td>-17.817177</td>\n",
       "      <td>-17.898476</td>\n",
       "      <td>-20.555359</td>\n",
       "      <td>-18.578117</td>\n",
       "      <td>-15.957151</td>\n",
       "      <td>...</td>\n",
       "      <td>-18.031603</td>\n",
       "      <td>-18.101122</td>\n",
       "      <td>-19.630695</td>\n",
       "      <td>-20.707493</td>\n",
       "      <td>-20.239124</td>\n",
       "      <td>-18.337662</td>\n",
       "      <td>-15.54331</td>\n",
       "      <td>-17.478794</td>\n",
       "      <td>-17.017057</td>\n",
       "      <td>-19.112486</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 216 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0          1          2          3          4          5    \\\n",
       "4950  -17.142696 -17.249537 -18.365582 -18.948351 -17.365459 -16.711090   \n",
       "3860  -18.354397 -20.836168 -22.571033 -22.155430 -20.623474 -17.927940   \n",
       "9761   -4.823565  -6.056048  -9.580621 -12.012063  -9.959867 -11.912548   \n",
       "7620   -7.031146  -4.253551  -4.534490  -5.836689  -5.248197  -6.456452   \n",
       "11586 -22.565975 -21.767015 -20.529488 -20.669310 -21.171085 -18.573399   \n",
       "7914  -20.082027 -18.982424 -17.009443 -16.944057 -19.334974 -19.527683   \n",
       "9513  -20.103537 -18.625866 -16.116108 -16.929594 -18.197668 -18.149664   \n",
       "5835  -21.078182 -18.671947 -17.676802 -18.009502 -18.218781 -18.676929   \n",
       "5389  -22.707445 -20.160748 -18.926155 -19.429979 -19.278114 -18.125767   \n",
       "11222 -20.008287 -17.640305 -19.188612 -18.435749 -17.538836 -17.817177   \n",
       "\n",
       "             6          7          8          9    ...        206        207  \\\n",
       "4950  -17.699482 -18.021383 -17.897398 -15.878503  ... -22.865425 -21.614164   \n",
       "3860  -15.832610 -18.623466 -21.543407 -24.989613  ...   0.000000   0.000000   \n",
       "9761  -13.994515 -13.555813 -14.022305 -15.118246  ...   0.000000   0.000000   \n",
       "7620   -8.122451  -9.154640  -8.647813  -8.178625  ...   0.000000   0.000000   \n",
       "11586 -18.412350 -16.178038 -14.222460 -15.122540  ...   0.000000   0.000000   \n",
       "7914  -21.974346 -20.144060 -17.954927 -19.302570  ...   0.000000   0.000000   \n",
       "9513  -19.240425 -18.361637 -16.917982 -16.639193  ... -17.848896 -19.357054   \n",
       "5835  -16.592762 -17.796528 -17.992470 -17.824917  ...   0.000000   0.000000   \n",
       "5389  -18.068565 -19.879320 -20.896755 -19.176098  ...   0.000000   0.000000   \n",
       "11222 -17.898476 -20.555359 -18.578117 -15.957151  ... -18.031603 -18.101122   \n",
       "\n",
       "             208        209        210        211       212        213  \\\n",
       "4950  -19.724932 -18.845335 -19.363422 -20.137630 -22.65514 -24.578310   \n",
       "3860    0.000000   0.000000   0.000000   0.000000   0.00000   0.000000   \n",
       "9761    0.000000   0.000000   0.000000   0.000000   0.00000   0.000000   \n",
       "7620    0.000000   0.000000   0.000000   0.000000   0.00000   0.000000   \n",
       "11586   0.000000   0.000000   0.000000   0.000000   0.00000   0.000000   \n",
       "7914    0.000000   0.000000   0.000000   0.000000   0.00000   0.000000   \n",
       "9513  -17.748692 -19.136808   0.000000   0.000000   0.00000   0.000000   \n",
       "5835    0.000000   0.000000   0.000000   0.000000   0.00000   0.000000   \n",
       "5389    0.000000   0.000000   0.000000   0.000000   0.00000   0.000000   \n",
       "11222 -19.630695 -20.707493 -20.239124 -18.337662 -15.54331 -17.478794   \n",
       "\n",
       "             214        215  \n",
       "4950  -24.039165 -23.209587  \n",
       "3860    0.000000   0.000000  \n",
       "9761    0.000000   0.000000  \n",
       "7620    0.000000   0.000000  \n",
       "11586   0.000000   0.000000  \n",
       "7914    0.000000   0.000000  \n",
       "9513    0.000000   0.000000  \n",
       "5835    0.000000   0.000000  \n",
       "5389    0.000000   0.000000  \n",
       "11222 -17.017057 -19.112486  \n",
       "\n",
       "[10 rows x 216 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split between train and test \n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop(['path','labels','source'],axis=1)\n",
    "                                                    , df.labels\n",
    "                                                    , test_size=0.25\n",
    "                                                    , shuffle=True\n",
    "                                                    , random_state=42\n",
    "                                                   )\n",
    "\n",
    "# Lets see how the data present itself before normalisation \n",
    "X_train[150:160]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now because we are mixing up a few different data sources, it would be wise to normalise the data. This is proven to improve the accuracy and speed up the training process. Prior to the discovery of this solution in the embrionic years of neural network, the problem used to be know as \"exploding gradients\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>206</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "      <th>213</th>\n",
       "      <th>214</th>\n",
       "      <th>215</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4950</th>\n",
       "      <td>0.373270</td>\n",
       "      <td>0.352227</td>\n",
       "      <td>0.438396</td>\n",
       "      <td>0.389021</td>\n",
       "      <td>0.498961</td>\n",
       "      <td>0.542144</td>\n",
       "      <td>0.461018</td>\n",
       "      <td>0.432419</td>\n",
       "      <td>0.435712</td>\n",
       "      <td>0.581127</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.874217</td>\n",
       "      <td>-0.805570</td>\n",
       "      <td>-0.685764</td>\n",
       "      <td>-0.625542</td>\n",
       "      <td>-0.671389</td>\n",
       "      <td>-0.713683</td>\n",
       "      <td>-0.856419</td>\n",
       "      <td>-0.980170</td>\n",
       "      <td>-0.965941</td>\n",
       "      <td>-0.910889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3860</th>\n",
       "      <td>0.286191</td>\n",
       "      <td>0.088576</td>\n",
       "      <td>0.127417</td>\n",
       "      <td>0.152359</td>\n",
       "      <td>0.258929</td>\n",
       "      <td>0.452538</td>\n",
       "      <td>0.598172</td>\n",
       "      <td>0.388228</td>\n",
       "      <td>0.168150</td>\n",
       "      <td>-0.086836</td>\n",
       "      <td>...</td>\n",
       "      <td>0.539838</td>\n",
       "      <td>0.525971</td>\n",
       "      <td>0.525370</td>\n",
       "      <td>0.526296</td>\n",
       "      <td>0.511449</td>\n",
       "      <td>0.511353</td>\n",
       "      <td>0.512023</td>\n",
       "      <td>0.500333</td>\n",
       "      <td>0.493808</td>\n",
       "      <td>0.487405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9761</th>\n",
       "      <td>1.258580</td>\n",
       "      <td>1.175052</td>\n",
       "      <td>1.088016</td>\n",
       "      <td>0.900874</td>\n",
       "      <td>1.044563</td>\n",
       "      <td>0.895499</td>\n",
       "      <td>0.733212</td>\n",
       "      <td>0.760179</td>\n",
       "      <td>0.720084</td>\n",
       "      <td>0.636864</td>\n",
       "      <td>...</td>\n",
       "      <td>0.539838</td>\n",
       "      <td>0.525971</td>\n",
       "      <td>0.525370</td>\n",
       "      <td>0.526296</td>\n",
       "      <td>0.511449</td>\n",
       "      <td>0.511353</td>\n",
       "      <td>0.512023</td>\n",
       "      <td>0.500333</td>\n",
       "      <td>0.493808</td>\n",
       "      <td>0.487405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7620</th>\n",
       "      <td>1.099933</td>\n",
       "      <td>1.307553</td>\n",
       "      <td>1.461161</td>\n",
       "      <td>1.356577</td>\n",
       "      <td>1.391693</td>\n",
       "      <td>1.297275</td>\n",
       "      <td>1.164616</td>\n",
       "      <td>1.083213</td>\n",
       "      <td>1.114490</td>\n",
       "      <td>1.145629</td>\n",
       "      <td>...</td>\n",
       "      <td>0.539838</td>\n",
       "      <td>0.525971</td>\n",
       "      <td>0.525370</td>\n",
       "      <td>0.526296</td>\n",
       "      <td>0.511449</td>\n",
       "      <td>0.511353</td>\n",
       "      <td>0.512023</td>\n",
       "      <td>0.500333</td>\n",
       "      <td>0.493808</td>\n",
       "      <td>0.487405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11586</th>\n",
       "      <td>-0.016472</td>\n",
       "      <td>0.020150</td>\n",
       "      <td>0.278383</td>\n",
       "      <td>0.262025</td>\n",
       "      <td>0.218584</td>\n",
       "      <td>0.405008</td>\n",
       "      <td>0.408646</td>\n",
       "      <td>0.567715</td>\n",
       "      <td>0.705396</td>\n",
       "      <td>0.636549</td>\n",
       "      <td>...</td>\n",
       "      <td>0.539838</td>\n",
       "      <td>0.525971</td>\n",
       "      <td>0.525370</td>\n",
       "      <td>0.526296</td>\n",
       "      <td>0.511449</td>\n",
       "      <td>0.511353</td>\n",
       "      <td>0.512023</td>\n",
       "      <td>0.500333</td>\n",
       "      <td>0.493808</td>\n",
       "      <td>0.487405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7914</th>\n",
       "      <td>0.162036</td>\n",
       "      <td>0.224843</td>\n",
       "      <td>0.538679</td>\n",
       "      <td>0.536925</td>\n",
       "      <td>0.353858</td>\n",
       "      <td>0.334736</td>\n",
       "      <td>0.146956</td>\n",
       "      <td>0.276621</td>\n",
       "      <td>0.431490</td>\n",
       "      <td>0.330098</td>\n",
       "      <td>...</td>\n",
       "      <td>0.539838</td>\n",
       "      <td>0.525971</td>\n",
       "      <td>0.525370</td>\n",
       "      <td>0.526296</td>\n",
       "      <td>0.511449</td>\n",
       "      <td>0.511353</td>\n",
       "      <td>0.512023</td>\n",
       "      <td>0.500333</td>\n",
       "      <td>0.493808</td>\n",
       "      <td>0.487405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9513</th>\n",
       "      <td>0.160490</td>\n",
       "      <td>0.251054</td>\n",
       "      <td>0.604738</td>\n",
       "      <td>0.537992</td>\n",
       "      <td>0.437648</td>\n",
       "      <td>0.436211</td>\n",
       "      <td>0.347810</td>\n",
       "      <td>0.407445</td>\n",
       "      <td>0.507586</td>\n",
       "      <td>0.525359</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.563982</td>\n",
       "      <td>-0.666520</td>\n",
       "      <td>-0.564420</td>\n",
       "      <td>-0.643357</td>\n",
       "      <td>0.511449</td>\n",
       "      <td>0.511353</td>\n",
       "      <td>0.512023</td>\n",
       "      <td>0.500333</td>\n",
       "      <td>0.493808</td>\n",
       "      <td>0.487405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5835</th>\n",
       "      <td>0.090447</td>\n",
       "      <td>0.247666</td>\n",
       "      <td>0.489330</td>\n",
       "      <td>0.458302</td>\n",
       "      <td>0.436093</td>\n",
       "      <td>0.397384</td>\n",
       "      <td>0.542326</td>\n",
       "      <td>0.448923</td>\n",
       "      <td>0.428735</td>\n",
       "      <td>0.438429</td>\n",
       "      <td>...</td>\n",
       "      <td>0.539838</td>\n",
       "      <td>0.525971</td>\n",
       "      <td>0.525370</td>\n",
       "      <td>0.526296</td>\n",
       "      <td>0.511449</td>\n",
       "      <td>0.511353</td>\n",
       "      <td>0.512023</td>\n",
       "      <td>0.500333</td>\n",
       "      <td>0.493808</td>\n",
       "      <td>0.487405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5389</th>\n",
       "      <td>-0.026639</td>\n",
       "      <td>0.138226</td>\n",
       "      <td>0.396944</td>\n",
       "      <td>0.353480</td>\n",
       "      <td>0.358047</td>\n",
       "      <td>0.437970</td>\n",
       "      <td>0.433903</td>\n",
       "      <td>0.296052</td>\n",
       "      <td>0.215605</td>\n",
       "      <td>0.339370</td>\n",
       "      <td>...</td>\n",
       "      <td>0.539838</td>\n",
       "      <td>0.525971</td>\n",
       "      <td>0.525370</td>\n",
       "      <td>0.526296</td>\n",
       "      <td>0.511449</td>\n",
       "      <td>0.511353</td>\n",
       "      <td>0.512023</td>\n",
       "      <td>0.500333</td>\n",
       "      <td>0.493808</td>\n",
       "      <td>0.487405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11222</th>\n",
       "      <td>0.167335</td>\n",
       "      <td>0.323502</td>\n",
       "      <td>0.377536</td>\n",
       "      <td>0.426848</td>\n",
       "      <td>0.486187</td>\n",
       "      <td>0.460694</td>\n",
       "      <td>0.446399</td>\n",
       "      <td>0.246432</td>\n",
       "      <td>0.385757</td>\n",
       "      <td>0.575361</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.575281</td>\n",
       "      <td>-0.589149</td>\n",
       "      <td>-0.679978</td>\n",
       "      <td>-0.739358</td>\n",
       "      <td>-0.724882</td>\n",
       "      <td>-0.604185</td>\n",
       "      <td>-0.426842</td>\n",
       "      <td>-0.552522</td>\n",
       "      <td>-0.539532</td>\n",
       "      <td>-0.664053</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 216 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6    \\\n",
       "4950   0.373270  0.352227  0.438396  0.389021  0.498961  0.542144  0.461018   \n",
       "3860   0.286191  0.088576  0.127417  0.152359  0.258929  0.452538  0.598172   \n",
       "9761   1.258580  1.175052  1.088016  0.900874  1.044563  0.895499  0.733212   \n",
       "7620   1.099933  1.307553  1.461161  1.356577  1.391693  1.297275  1.164616   \n",
       "11586 -0.016472  0.020150  0.278383  0.262025  0.218584  0.405008  0.408646   \n",
       "7914   0.162036  0.224843  0.538679  0.536925  0.353858  0.334736  0.146956   \n",
       "9513   0.160490  0.251054  0.604738  0.537992  0.437648  0.436211  0.347810   \n",
       "5835   0.090447  0.247666  0.489330  0.458302  0.436093  0.397384  0.542326   \n",
       "5389  -0.026639  0.138226  0.396944  0.353480  0.358047  0.437970  0.433903   \n",
       "11222  0.167335  0.323502  0.377536  0.426848  0.486187  0.460694  0.446399   \n",
       "\n",
       "            7         8         9    ...       206       207       208  \\\n",
       "4950   0.432419  0.435712  0.581127  ... -0.874217 -0.805570 -0.685764   \n",
       "3860   0.388228  0.168150 -0.086836  ...  0.539838  0.525971  0.525370   \n",
       "9761   0.760179  0.720084  0.636864  ...  0.539838  0.525971  0.525370   \n",
       "7620   1.083213  1.114490  1.145629  ...  0.539838  0.525971  0.525370   \n",
       "11586  0.567715  0.705396  0.636549  ...  0.539838  0.525971  0.525370   \n",
       "7914   0.276621  0.431490  0.330098  ...  0.539838  0.525971  0.525370   \n",
       "9513   0.407445  0.507586  0.525359  ... -0.563982 -0.666520 -0.564420   \n",
       "5835   0.448923  0.428735  0.438429  ...  0.539838  0.525971  0.525370   \n",
       "5389   0.296052  0.215605  0.339370  ...  0.539838  0.525971  0.525370   \n",
       "11222  0.246432  0.385757  0.575361  ... -0.575281 -0.589149 -0.679978   \n",
       "\n",
       "            209       210       211       212       213       214       215  \n",
       "4950  -0.625542 -0.671389 -0.713683 -0.856419 -0.980170 -0.965941 -0.910889  \n",
       "3860   0.526296  0.511449  0.511353  0.512023  0.500333  0.493808  0.487405  \n",
       "9761   0.526296  0.511449  0.511353  0.512023  0.500333  0.493808  0.487405  \n",
       "7620   0.526296  0.511449  0.511353  0.512023  0.500333  0.493808  0.487405  \n",
       "11586  0.526296  0.511449  0.511353  0.512023  0.500333  0.493808  0.487405  \n",
       "7914   0.526296  0.511449  0.511353  0.512023  0.500333  0.493808  0.487405  \n",
       "9513  -0.643357  0.511449  0.511353  0.512023  0.500333  0.493808  0.487405  \n",
       "5835   0.526296  0.511449  0.511353  0.512023  0.500333  0.493808  0.487405  \n",
       "5389   0.526296  0.511449  0.511353  0.512023  0.500333  0.493808  0.487405  \n",
       "11222 -0.739358 -0.724882 -0.604185 -0.426842 -0.552522 -0.539532 -0.664053  \n",
       "\n",
       "[10 rows x 216 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lts do data normalization \n",
    "mean = np.mean(X_train, axis=0)\n",
    "std = np.std(X_train, axis=0)\n",
    "\n",
    "X_train = (X_train - mean)/std\n",
    "X_test = (X_test - mean)/std\n",
    "\n",
    "# Check the dataset now \n",
    "X_train[150:160]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of documentation, I'll just mention here that there's another method for normalisation but it hasn't worked out well, at least not when I implemented it. So I swapped in for something more simple, which is what i've implemented above. Perhaps someone else could give it a try below\n",
    "\n",
    "```python\n",
    "max_data = np.max(X_train)\n",
    "min_data = np.min(X_train)\n",
    "X_train = (X_train-min_data)/(max_data-min_data+1e-6)\n",
    "X_train =  X_train-0.5\n",
    "\n",
    "max_data = np.max(X_test)\n",
    "min_data = np.min(X_test)\n",
    "X_test = (X_test-min_data)/(max_data-min_data+1e-6)\n",
    "X_test =  X_test-0.5\n",
    "\n",
    "X_train[150:160]\n",
    "```\n",
    "\n",
    "Next part we'll need to convert the data format to a numpy array, because we are using keras. Initially I had plans to use XGboost or LightGBM for this task. But since I've potential plans to move to a 2D CNN, it may make sense to continue on the Deep Learning path way and implement a ID CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9121, 216)\n",
      "['female_angry' 'female_disgust' 'female_fear' 'female_happy'\n",
      " 'female_neutral' 'female_sad' 'female_surprise' 'male_angry'\n",
      " 'male_disgust' 'male_fear' 'male_happy' 'male_neutral' 'male_sad'\n",
      " 'male_surprise']\n"
     ]
    }
   ],
   "source": [
    "# Lets few preparation steps to get it into the correct format for Keras \n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# one hot encode the target \n",
    "lb = LabelEncoder()\n",
    "y_train = to_categorical(lb.fit_transform(y_train))\n",
    "y_test = to_categorical(lb.fit_transform(y_test))\n",
    "\n",
    "print(X_train.shape)\n",
    "print(lb.classes_)\n",
    "#print(y_train[0:10])\n",
    "#print(y_test[0:10])\n",
    "\n",
    "# Pickel the lb object for future use \n",
    "filename = 'labels'\n",
    "outfile = open(filename,'wb')\n",
    "pickle.dump(lb,outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------\n",
    "Now because we are using a CNN, we need to specify the 3rd dimension, which for us is 1. Its 1 because we're doing a 1D CNN and not a 2D CNN. If we use the MFCC data in its entirity, we could feed that through as the input data, thus making the network a 2D CNN.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9121, 216, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = np.expand_dims(X_train, axis=2)\n",
    "X_test = np.expand_dims(X_test, axis=2)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"modelling\"></a>\n",
    "## 2. Modelling\n",
    "The architecture of the model below is based on a few sources that I've seen before such as Kaggle and Stackoverflow. I'm unable to find the source but safe to say this particular format works quite well and is fast, although I've used GPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.RMSprop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_16 (Conv1D)          (None, 216, 256)          2304      \n",
      "                                                                 \n",
      " activation_18 (Activation)  (None, 216, 256)          0         \n",
      "                                                                 \n",
      " conv1d_17 (Conv1D)          (None, 216, 256)          524544    \n",
      "                                                                 \n",
      " batch_normalization_4 (Bat  (None, 216, 256)          1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_19 (Activation)  (None, 216, 256)          0         \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 216, 256)          0         \n",
      "                                                                 \n",
      " max_pooling1d_4 (MaxPoolin  (None, 27, 256)           0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " conv1d_18 (Conv1D)          (None, 27, 128)           262272    \n",
      "                                                                 \n",
      " activation_20 (Activation)  (None, 27, 128)           0         \n",
      "                                                                 \n",
      " conv1d_19 (Conv1D)          (None, 27, 128)           131200    \n",
      "                                                                 \n",
      " activation_21 (Activation)  (None, 27, 128)           0         \n",
      "                                                                 \n",
      " conv1d_20 (Conv1D)          (None, 27, 128)           131200    \n",
      "                                                                 \n",
      " activation_22 (Activation)  (None, 27, 128)           0         \n",
      "                                                                 \n",
      " conv1d_21 (Conv1D)          (None, 27, 128)           131200    \n",
      "                                                                 \n",
      " batch_normalization_5 (Bat  (None, 27, 128)           512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_23 (Activation)  (None, 27, 128)           0         \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 27, 128)           0         \n",
      "                                                                 \n",
      " max_pooling1d_5 (MaxPoolin  (None, 3, 128)            0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " conv1d_22 (Conv1D)          (None, 3, 64)             65600     \n",
      "                                                                 \n",
      " activation_24 (Activation)  (None, 3, 64)             0         \n",
      "                                                                 \n",
      " conv1d_23 (Conv1D)          (None, 3, 64)             32832     \n",
      "                                                                 \n",
      " activation_25 (Activation)  (None, 3, 64)             0         \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 192)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 14)                2702      \n",
      "                                                                 \n",
      " activation_26 (Activation)  (None, 14)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1285390 (4.90 MB)\n",
      "Trainable params: 1284622 (4.90 MB)\n",
      "Non-trainable params: 768 (3.00 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# New model\n",
    "model = Sequential()\n",
    "model.add(Conv1D(256, 8, padding='same',input_shape=(X_train.shape[1],1)))  # X_train.shape[1] = No. of Columns\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv1D(256, 8, padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(MaxPooling1D(pool_size=(8)))\n",
    "model.add(Conv1D(128, 8, padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv1D(128, 8, padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv1D(128, 8, padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv1D(128, 8, padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(MaxPooling1D(pool_size=(8)))\n",
    "model.add(Conv1D(64, 8, padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv1D(64, 8, padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(14)) # Target class number\n",
    "model.add(Activation('softmax'))\n",
    "# opt = keras.optimizers.SGD(lr=0.0001, momentum=0.0, decay=0.0, nesterov=False)\n",
    "# opt = keras.optimizers.Adam(lr=0.0001)\n",
    "opt = keras.optimizers.RMSprop(lr=0.00001)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "571/571 [==============================] - 31s 53ms/step - loss: 2.2083 - accuracy: 0.2165 - val_loss: 2.3672 - val_accuracy: 0.1394\n",
      "Epoch 2/100\n",
      "571/571 [==============================] - 31s 54ms/step - loss: 2.0141 - accuracy: 0.2812 - val_loss: 2.1604 - val_accuracy: 0.2532\n",
      "Epoch 3/100\n",
      "571/571 [==============================] - 31s 55ms/step - loss: 1.9185 - accuracy: 0.3167 - val_loss: 2.0331 - val_accuracy: 0.2677\n",
      "Epoch 4/100\n",
      "571/571 [==============================] - 30s 53ms/step - loss: 1.8360 - accuracy: 0.3507 - val_loss: 1.9133 - val_accuracy: 0.3035\n",
      "Epoch 5/100\n",
      "571/571 [==============================] - 30s 52ms/step - loss: 1.7849 - accuracy: 0.3657 - val_loss: 1.8540 - val_accuracy: 0.3288\n",
      "Epoch 6/100\n",
      "571/571 [==============================] - 30s 53ms/step - loss: 1.7601 - accuracy: 0.3802 - val_loss: 1.8979 - val_accuracy: 0.3239\n",
      "Epoch 7/100\n",
      "571/571 [==============================] - 30s 53ms/step - loss: 1.7162 - accuracy: 0.3924 - val_loss: 1.9163 - val_accuracy: 0.3338\n",
      "Epoch 8/100\n",
      "571/571 [==============================] - 30s 53ms/step - loss: 1.7055 - accuracy: 0.3984 - val_loss: 1.9608 - val_accuracy: 0.3275\n",
      "Epoch 9/100\n",
      "571/571 [==============================] - 31s 53ms/step - loss: 1.6887 - accuracy: 0.4110 - val_loss: 1.8019 - val_accuracy: 0.3663\n",
      "Epoch 10/100\n",
      "571/571 [==============================] - 30s 53ms/step - loss: 1.6723 - accuracy: 0.4087 - val_loss: 1.8343 - val_accuracy: 0.3407\n",
      "Epoch 11/100\n",
      "571/571 [==============================] - 30s 53ms/step - loss: 1.6526 - accuracy: 0.4209 - val_loss: 1.7207 - val_accuracy: 0.3923\n",
      "Epoch 12/100\n",
      "571/571 [==============================] - 31s 54ms/step - loss: 1.6427 - accuracy: 0.4184 - val_loss: 1.8383 - val_accuracy: 0.3706\n",
      "Epoch 13/100\n",
      "571/571 [==============================] - 30s 53ms/step - loss: 1.6305 - accuracy: 0.4353 - val_loss: 2.0452 - val_accuracy: 0.2887\n",
      "Epoch 14/100\n",
      "571/571 [==============================] - 31s 54ms/step - loss: 1.6398 - accuracy: 0.4254 - val_loss: 1.7517 - val_accuracy: 0.3762\n",
      "Epoch 15/100\n",
      "571/571 [==============================] - 31s 53ms/step - loss: 1.6227 - accuracy: 0.4300 - val_loss: 1.8797 - val_accuracy: 0.3450\n",
      "Epoch 16/100\n",
      "571/571 [==============================] - 31s 54ms/step - loss: 1.6144 - accuracy: 0.4371 - val_loss: 1.6733 - val_accuracy: 0.4061\n",
      "Epoch 17/100\n",
      "571/571 [==============================] - 31s 55ms/step - loss: 1.6062 - accuracy: 0.4353 - val_loss: 1.8214 - val_accuracy: 0.3775\n",
      "Epoch 18/100\n",
      "571/571 [==============================] - 30s 53ms/step - loss: 1.6057 - accuracy: 0.4366 - val_loss: 1.7848 - val_accuracy: 0.3831\n",
      "Epoch 19/100\n",
      "571/571 [==============================] - 30s 53ms/step - loss: 1.5990 - accuracy: 0.4451 - val_loss: 2.4717 - val_accuracy: 0.2996\n",
      "Epoch 20/100\n",
      "571/571 [==============================] - 30s 53ms/step - loss: 1.5922 - accuracy: 0.4428 - val_loss: 1.6922 - val_accuracy: 0.4176\n",
      "Epoch 21/100\n",
      "571/571 [==============================] - 31s 54ms/step - loss: 1.5862 - accuracy: 0.4487 - val_loss: 1.7729 - val_accuracy: 0.3759\n",
      "Epoch 22/100\n",
      "571/571 [==============================] - 31s 54ms/step - loss: 1.5765 - accuracy: 0.4463 - val_loss: 1.8794 - val_accuracy: 0.3680\n",
      "Epoch 23/100\n",
      "571/571 [==============================] - 31s 54ms/step - loss: 1.5836 - accuracy: 0.4495 - val_loss: 1.7499 - val_accuracy: 0.4005\n",
      "Epoch 24/100\n",
      "571/571 [==============================] - 32s 56ms/step - loss: 1.5739 - accuracy: 0.4544 - val_loss: 1.7266 - val_accuracy: 0.4035\n",
      "Epoch 25/100\n",
      "571/571 [==============================] - 31s 54ms/step - loss: 1.5738 - accuracy: 0.4546 - val_loss: 1.7748 - val_accuracy: 0.3861\n",
      "Epoch 26/100\n",
      "571/571 [==============================] - 32s 56ms/step - loss: 1.5640 - accuracy: 0.4543 - val_loss: 1.8054 - val_accuracy: 0.3719\n",
      "Epoch 27/100\n",
      "571/571 [==============================] - 31s 54ms/step - loss: 1.5714 - accuracy: 0.4529 - val_loss: 1.7397 - val_accuracy: 0.3953\n",
      "Epoch 28/100\n",
      "571/571 [==============================] - 31s 54ms/step - loss: 1.5633 - accuracy: 0.4573 - val_loss: 1.8764 - val_accuracy: 0.3538\n",
      "Epoch 29/100\n",
      "571/571 [==============================] - 32s 55ms/step - loss: 1.5573 - accuracy: 0.4572 - val_loss: 1.9234 - val_accuracy: 0.3456\n",
      "Epoch 30/100\n",
      "571/571 [==============================] - 31s 55ms/step - loss: 1.5481 - accuracy: 0.4661 - val_loss: 2.2791 - val_accuracy: 0.3331\n",
      "Epoch 31/100\n",
      "571/571 [==============================] - 31s 55ms/step - loss: 1.5594 - accuracy: 0.4593 - val_loss: 1.7867 - val_accuracy: 0.3989\n",
      "Epoch 32/100\n",
      "571/571 [==============================] - 31s 54ms/step - loss: 1.5469 - accuracy: 0.4532 - val_loss: 1.7630 - val_accuracy: 0.4002\n",
      "Epoch 33/100\n",
      "571/571 [==============================] - 31s 54ms/step - loss: 1.5277 - accuracy: 0.4653 - val_loss: 1.9694 - val_accuracy: 0.3295\n",
      "Epoch 34/100\n",
      "571/571 [==============================] - 32s 56ms/step - loss: 1.5499 - accuracy: 0.4570 - val_loss: 1.7459 - val_accuracy: 0.3972\n",
      "Epoch 35/100\n",
      "571/571 [==============================] - 31s 55ms/step - loss: 1.5331 - accuracy: 0.4688 - val_loss: 1.7974 - val_accuracy: 0.3699\n",
      "Epoch 36/100\n",
      "571/571 [==============================] - 30s 53ms/step - loss: 1.5243 - accuracy: 0.4690 - val_loss: 1.7189 - val_accuracy: 0.4087\n",
      "Epoch 37/100\n",
      "571/571 [==============================] - 31s 54ms/step - loss: 1.5386 - accuracy: 0.4669 - val_loss: 1.8011 - val_accuracy: 0.3762\n",
      "Epoch 38/100\n",
      "571/571 [==============================] - 31s 54ms/step - loss: 1.5261 - accuracy: 0.4700 - val_loss: 1.7654 - val_accuracy: 0.3880\n",
      "Epoch 39/100\n",
      "571/571 [==============================] - 33s 58ms/step - loss: 1.5300 - accuracy: 0.4631 - val_loss: 1.9285 - val_accuracy: 0.3528\n",
      "Epoch 40/100\n",
      "571/571 [==============================] - 36s 63ms/step - loss: 1.5259 - accuracy: 0.4631 - val_loss: 1.8820 - val_accuracy: 0.3568\n",
      "Epoch 41/100\n",
      "571/571 [==============================] - 36s 63ms/step - loss: 1.5191 - accuracy: 0.4715 - val_loss: 1.9169 - val_accuracy: 0.3601\n",
      "Epoch 42/100\n",
      "571/571 [==============================] - 31s 55ms/step - loss: 1.5147 - accuracy: 0.4700 - val_loss: 1.7605 - val_accuracy: 0.3828\n",
      "Epoch 43/100\n",
      "571/571 [==============================] - 31s 54ms/step - loss: 1.5169 - accuracy: 0.4735 - val_loss: 1.7561 - val_accuracy: 0.3946\n",
      "Epoch 44/100\n",
      "571/571 [==============================] - 31s 54ms/step - loss: 1.5208 - accuracy: 0.4766 - val_loss: 1.8400 - val_accuracy: 0.3588\n",
      "Epoch 45/100\n",
      "571/571 [==============================] - 30s 53ms/step - loss: 1.5099 - accuracy: 0.4703 - val_loss: 1.8193 - val_accuracy: 0.3690\n",
      "Epoch 46/100\n",
      "571/571 [==============================] - 32s 55ms/step - loss: 1.4947 - accuracy: 0.4800 - val_loss: 1.8118 - val_accuracy: 0.3749\n",
      "Epoch 47/100\n",
      "571/571 [==============================] - 31s 54ms/step - loss: 1.5078 - accuracy: 0.4827 - val_loss: 1.8071 - val_accuracy: 0.3699\n",
      "Epoch 48/100\n",
      "571/571 [==============================] - 31s 54ms/step - loss: 1.5121 - accuracy: 0.4802 - val_loss: 1.8821 - val_accuracy: 0.3479\n",
      "Epoch 49/100\n",
      "571/571 [==============================] - 32s 56ms/step - loss: 1.5000 - accuracy: 0.4840 - val_loss: 1.7715 - val_accuracy: 0.3834\n",
      "Epoch 50/100\n",
      "571/571 [==============================] - 32s 55ms/step - loss: 1.4814 - accuracy: 0.4792 - val_loss: 1.9437 - val_accuracy: 0.3696\n",
      "Epoch 51/100\n",
      "571/571 [==============================] - 32s 55ms/step - loss: 1.5059 - accuracy: 0.4754 - val_loss: 2.0311 - val_accuracy: 0.3134\n",
      "Epoch 52/100\n",
      "571/571 [==============================] - 31s 55ms/step - loss: 1.5048 - accuracy: 0.4791 - val_loss: 1.8568 - val_accuracy: 0.3841\n",
      "Epoch 53/100\n",
      "571/571 [==============================] - 31s 54ms/step - loss: 1.4809 - accuracy: 0.4853 - val_loss: 1.8299 - val_accuracy: 0.3959\n",
      "Epoch 54/100\n",
      "571/571 [==============================] - 31s 55ms/step - loss: 1.4925 - accuracy: 0.4790 - val_loss: 1.7729 - val_accuracy: 0.3933\n",
      "Epoch 55/100\n",
      "571/571 [==============================] - 32s 56ms/step - loss: 1.4806 - accuracy: 0.4855 - val_loss: 2.4256 - val_accuracy: 0.3259\n",
      "Epoch 56/100\n",
      "571/571 [==============================] - 31s 54ms/step - loss: 1.4824 - accuracy: 0.4891 - val_loss: 1.8653 - val_accuracy: 0.3502\n",
      "Epoch 57/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "571/571 [==============================] - 32s 56ms/step - loss: 1.4716 - accuracy: 0.4851 - val_loss: 1.8867 - val_accuracy: 0.3604\n",
      "Epoch 58/100\n",
      "571/571 [==============================] - 32s 56ms/step - loss: 1.4892 - accuracy: 0.4774 - val_loss: 1.9868 - val_accuracy: 0.3634\n",
      "Epoch 59/100\n",
      "571/571 [==============================] - 32s 56ms/step - loss: 1.4764 - accuracy: 0.4881 - val_loss: 1.8712 - val_accuracy: 0.3716\n",
      "Epoch 60/100\n",
      "571/571 [==============================] - 32s 55ms/step - loss: 1.4637 - accuracy: 0.4929 - val_loss: 1.9274 - val_accuracy: 0.3459\n",
      "Epoch 61/100\n",
      "571/571 [==============================] - 33s 57ms/step - loss: 1.4682 - accuracy: 0.4904 - val_loss: 1.8181 - val_accuracy: 0.3815\n",
      "Epoch 62/100\n",
      "571/571 [==============================] - 32s 55ms/step - loss: 1.4740 - accuracy: 0.4922 - val_loss: 2.0351 - val_accuracy: 0.3249\n",
      "Epoch 63/100\n",
      "571/571 [==============================] - 32s 56ms/step - loss: 1.4688 - accuracy: 0.4949 - val_loss: 1.8362 - val_accuracy: 0.3696\n",
      "Epoch 64/100\n",
      "571/571 [==============================] - 32s 56ms/step - loss: 1.4548 - accuracy: 0.4935 - val_loss: 1.9497 - val_accuracy: 0.3778\n",
      "Epoch 65/100\n",
      "571/571 [==============================] - 31s 55ms/step - loss: 1.4724 - accuracy: 0.4926 - val_loss: 1.9660 - val_accuracy: 0.3476\n",
      "Epoch 66/100\n",
      "571/571 [==============================] - 31s 54ms/step - loss: 1.4575 - accuracy: 0.4907 - val_loss: 1.9616 - val_accuracy: 0.3298\n",
      "Epoch 67/100\n",
      "571/571 [==============================] - 31s 55ms/step - loss: 1.4777 - accuracy: 0.4978 - val_loss: 1.8303 - val_accuracy: 0.3874\n",
      "Epoch 68/100\n",
      "571/571 [==============================] - 31s 54ms/step - loss: 1.4687 - accuracy: 0.4995 - val_loss: 1.8550 - val_accuracy: 0.4055\n",
      "Epoch 69/100\n",
      "571/571 [==============================] - 31s 54ms/step - loss: 1.4437 - accuracy: 0.4985 - val_loss: 2.0757 - val_accuracy: 0.3154\n",
      "Epoch 70/100\n",
      "571/571 [==============================] - 30s 53ms/step - loss: 1.4481 - accuracy: 0.4982 - val_loss: 1.8211 - val_accuracy: 0.3801\n",
      "Epoch 71/100\n",
      "571/571 [==============================] - 31s 55ms/step - loss: 1.4404 - accuracy: 0.4984 - val_loss: 2.0018 - val_accuracy: 0.3607\n",
      "Epoch 72/100\n",
      "571/571 [==============================] - 32s 55ms/step - loss: 1.4563 - accuracy: 0.4955 - val_loss: 1.9729 - val_accuracy: 0.3574\n",
      "Epoch 73/100\n",
      "571/571 [==============================] - 33s 58ms/step - loss: 1.4460 - accuracy: 0.4959 - val_loss: 1.9966 - val_accuracy: 0.3019\n",
      "Epoch 74/100\n",
      "571/571 [==============================] - 32s 56ms/step - loss: 1.4326 - accuracy: 0.5070 - val_loss: 2.3085 - val_accuracy: 0.3818\n",
      "Epoch 75/100\n",
      "571/571 [==============================] - 32s 56ms/step - loss: 1.4529 - accuracy: 0.5021 - val_loss: 1.8541 - val_accuracy: 0.3617\n",
      "Epoch 76/100\n",
      "571/571 [==============================] - 33s 57ms/step - loss: 1.4575 - accuracy: 0.4946 - val_loss: 1.9061 - val_accuracy: 0.3798\n",
      "Epoch 77/100\n",
      "571/571 [==============================] - 32s 55ms/step - loss: 1.4504 - accuracy: 0.5019 - val_loss: 1.8629 - val_accuracy: 0.3607\n",
      "Epoch 78/100\n",
      "571/571 [==============================] - 32s 57ms/step - loss: 1.4385 - accuracy: 0.5006 - val_loss: 2.2429 - val_accuracy: 0.3505\n",
      "Epoch 79/100\n",
      "571/571 [==============================] - 32s 57ms/step - loss: 1.4440 - accuracy: 0.4990 - val_loss: 1.9130 - val_accuracy: 0.3269\n",
      "Epoch 80/100\n",
      "571/571 [==============================] - 31s 55ms/step - loss: 1.4437 - accuracy: 0.5030 - val_loss: 1.8536 - val_accuracy: 0.3857\n",
      "Epoch 81/100\n",
      "571/571 [==============================] - 31s 54ms/step - loss: 1.4323 - accuracy: 0.5069 - val_loss: 1.9578 - val_accuracy: 0.3745\n",
      "Epoch 82/100\n",
      "571/571 [==============================] - 30s 53ms/step - loss: 1.4432 - accuracy: 0.5031 - val_loss: 1.8545 - val_accuracy: 0.3792\n",
      "Epoch 83/100\n",
      "571/571 [==============================] - 30s 53ms/step - loss: 1.4298 - accuracy: 0.5054 - val_loss: 1.8288 - val_accuracy: 0.3815\n",
      "Epoch 84/100\n",
      "571/571 [==============================] - 30s 53ms/step - loss: 1.4220 - accuracy: 0.4999 - val_loss: 2.3355 - val_accuracy: 0.2953\n",
      "Epoch 85/100\n",
      "571/571 [==============================] - 31s 54ms/step - loss: 1.4218 - accuracy: 0.5074 - val_loss: 1.9353 - val_accuracy: 0.3496\n",
      "Epoch 86/100\n",
      "571/571 [==============================] - 31s 54ms/step - loss: 1.4191 - accuracy: 0.5093 - val_loss: 2.2748 - val_accuracy: 0.3371\n",
      "Epoch 87/100\n",
      "571/571 [==============================] - 31s 54ms/step - loss: 1.4053 - accuracy: 0.5107 - val_loss: 1.9397 - val_accuracy: 0.3430\n",
      "Epoch 88/100\n",
      "571/571 [==============================] - 34s 60ms/step - loss: 1.4093 - accuracy: 0.5144 - val_loss: 2.0190 - val_accuracy: 0.3282\n",
      "Epoch 89/100\n",
      "571/571 [==============================] - 32s 56ms/step - loss: 1.4093 - accuracy: 0.5115 - val_loss: 1.9585 - val_accuracy: 0.3870\n",
      "Epoch 90/100\n",
      "571/571 [==============================] - 31s 55ms/step - loss: 1.4172 - accuracy: 0.5110 - val_loss: 1.9847 - val_accuracy: 0.3801\n",
      "Epoch 91/100\n",
      "571/571 [==============================] - 31s 54ms/step - loss: 1.4219 - accuracy: 0.5112 - val_loss: 1.9624 - val_accuracy: 0.3499\n",
      "Epoch 92/100\n",
      "571/571 [==============================] - 33s 58ms/step - loss: 1.4008 - accuracy: 0.5160 - val_loss: 1.9382 - val_accuracy: 0.3453\n",
      "Epoch 93/100\n",
      "571/571 [==============================] - 32s 55ms/step - loss: 1.3860 - accuracy: 0.5142 - val_loss: 1.9714 - val_accuracy: 0.3459\n",
      "Epoch 94/100\n",
      "571/571 [==============================] - 31s 55ms/step - loss: 1.4132 - accuracy: 0.5110 - val_loss: 1.8288 - val_accuracy: 0.3811\n",
      "Epoch 95/100\n",
      "571/571 [==============================] - 32s 55ms/step - loss: 1.4025 - accuracy: 0.5144 - val_loss: 2.0164 - val_accuracy: 0.3542\n",
      "Epoch 96/100\n",
      "571/571 [==============================] - 31s 54ms/step - loss: 1.3930 - accuracy: 0.5173 - val_loss: 1.9457 - val_accuracy: 0.3479\n",
      "Epoch 97/100\n",
      "571/571 [==============================] - 31s 54ms/step - loss: 1.4022 - accuracy: 0.5152 - val_loss: 2.0220 - val_accuracy: 0.3732\n",
      "Epoch 98/100\n",
      "571/571 [==============================] - 31s 54ms/step - loss: 1.4437 - accuracy: 0.5146 - val_loss: 2.4137 - val_accuracy: 0.3239\n",
      "Epoch 99/100\n",
      "571/571 [==============================] - 31s 54ms/step - loss: 1.3877 - accuracy: 0.5227 - val_loss: 1.9574 - val_accuracy: 0.3667\n",
      "Epoch 100/100\n",
      "571/571 [==============================] - 32s 56ms/step - loss: 1.3912 - accuracy: 0.5201 - val_loss: 1.8926 - val_accuracy: 0.3515\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=opt,metrics=['accuracy'])\n",
    "model_history=model.fit(X_train, y_train, batch_size=16, epochs=100, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(model_history.history['loss'])\n",
    "plt.plot(model_history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see that the loss starts to plateau now at around 50 epochs. Regardless we'll keep it at 100 as the final model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"serialise\"></a>\n",
    "## 3. Model serialisation\n",
    "So its time to serialise the model for re-usability. Serialisation and saving mean the same thing. We need to serialise the model architecture and the weights, thats all. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save model and weights at /Users/sayalidalvi/ashritha/AudioJournaling/model_training/saved_models/Emotion_Model.h5 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sayalidalvi/ashritha/Assignment3/venv/lib/python3.9/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# Save model and weights\n",
    "model_name = 'Emotion_Model.h5'\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)\n",
    "print('Save model and weights at %s ' % model_path)\n",
    "\n",
    "# Save the model to disk\n",
    "model_json = model.to_json()\n",
    "with open(\"model_json.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"validation\"></a>\n",
    "## 4. Model validation\n",
    "Now predicting emotions on the test data. After serialising the model above, i'm going to just reload it into disk. Essentially to re-use the model without having to retrain by re-running the code, we just need to run this section of the code and apply the model to a new dataset. Since we used the same test set in the keras model, the result is essentially the same as the last epoch of 100 which is 43.80%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.RMSprop.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "accuracy: 35.15%\n"
     ]
    }
   ],
   "source": [
    "# loading json and model architecture \n",
    "json_file = open('model_json.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"saved_models/Emotion_Model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    " \n",
    "# Keras optimiser\n",
    "opt = keras.optimizers.RMSprop(lr=0.00001)\n",
    "loaded_model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "score = loaded_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = loaded_model.predict(X_test, \n",
    "                         batch_size=16, \n",
    "                         verbose=1)\n",
    "\n",
    "preds=preds.argmax(axis=1)\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction is in the form of numbers, we'll need to append the labels to it before we run the accuracy measure..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions \n",
    "preds = preds.astype(int).flatten()\n",
    "preds = (lb.inverse_transform((preds)))\n",
    "preds = pd.DataFrame({'predictedvalues': preds})\n",
    "\n",
    "# Actual labels\n",
    "actual=y_test.argmax(axis=1)\n",
    "actual = actual.astype(int).flatten()\n",
    "actual = (lb.inverse_transform((actual)))\n",
    "actual = pd.DataFrame({'actualvalues': actual})\n",
    "\n",
    "# Lets combined both of them into a single dataframe\n",
    "finaldf = actual.join(preds)\n",
    "finaldf[170:180]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets write the predictions out into a file for re-use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write out the predictions to disk\n",
    "finaldf.to_csv('Predictions.csv', index=False)\n",
    "finaldf.groupby('predictedvalues').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we've made our predictions, so how well have we done? We're going to use the most simplest form of accuracy measure which is absolute accuracy, which is really just the % of records where Actual = Predicted, over the total number of records predicted. We'll also produce the F1, recall and precision scores. \n",
    "\n",
    "The most common way to visualise this output is via a confusion matrix. I found an excellent heat map plot to visualise the accuracy of the confusion matrix [here](https://gist.github.com/shaypal5/94c53d765083101efc0240d776a23823) which i've borrowed for this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the confusion matrix heat map plot\n",
    "def print_confusion_matrix(confusion_matrix, class_names, figsize = (10,7), fontsize=14):\n",
    "    \"\"\"Prints a confusion matrix, as returned by sklearn.metrics.confusion_matrix, as a heatmap.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    confusion_matrix: numpy.ndarray\n",
    "        The numpy.ndarray object returned from a call to sklearn.metrics.confusion_matrix. \n",
    "        Similarly constructed ndarrays can also be used.\n",
    "    class_names: list\n",
    "        An ordered list of class names, in the order they index the given confusion matrix.\n",
    "    figsize: tuple\n",
    "        A 2-long tuple, the first value determining the horizontal size of the ouputted figure,\n",
    "        the second determining the vertical size. Defaults to (10,7).\n",
    "    fontsize: int\n",
    "        Font size for axes labels. Defaults to 14.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    matplotlib.figure.Figure\n",
    "        The resulting confusion matrix figure\n",
    "    \"\"\"\n",
    "    df_cm = pd.DataFrame(\n",
    "        confusion_matrix, index=class_names, columns=class_names, \n",
    "    )\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    try:\n",
    "        heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\")\n",
    "    except ValueError:\n",
    "        raise ValueError(\"Confusion matrix values must be integers.\")\n",
    "        \n",
    "    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=fontsize)\n",
    "    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "# Gender recode function\n",
    "def gender(row):\n",
    "    if row == 'female_disgust' or 'female_fear' or 'female_happy' or 'female_sad' or 'female_surprise' or 'female_neutral':\n",
    "        return 'female'\n",
    "    elif row == 'male_angry' or 'male_fear' or 'male_happy' or 'male_sad' or 'male_surprise' or 'male_neutral' or 'male_disgust':\n",
    "        return 'male'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Emotion by gender accuracy  \n",
    "So lets visualise how well we have done for the Emotion by Gender model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predictions file \n",
    "finaldf = pd.read_csv(\"Predictions.csv\")\n",
    "classes = finaldf.actualvalues.unique()\n",
    "classes.sort()    \n",
    "\n",
    "# Confusion matrix \n",
    "c = confusion_matrix(finaldf.actualvalues, finaldf.predictedvalues)\n",
    "print(accuracy_score(finaldf.actualvalues, finaldf.predictedvalues))\n",
    "print_confusion_matrix(c, class_names = classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report \n",
    "classes = finaldf.actualvalues.unique()\n",
    "classes.sort()    \n",
    "print(classification_report(finaldf.actualvalues, finaldf.predictedvalues, target_names=classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Absolute accuracy for the gender by emotions is 43%. Whilst that may not seem high at first but remember, a random guess correct is 1 out of 14 which is 7%. So 43% is huge! The heat map plot below will do justice in illustrating how good the results is. And note we have only just scratched the surface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------\n",
    "#### Gender accuracy result \n",
    "if you notice, that the gender classification is more accurate. So lets group them up and measure the accuracy again?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modidf = finaldf\n",
    "modidf['actualvalues'] = finaldf.actualvalues.replace({'female_angry':'female'\n",
    "                                       , 'female_disgust':'female'\n",
    "                                       , 'female_fear':'female'\n",
    "                                       , 'female_happy':'female'\n",
    "                                       , 'female_sad':'female'\n",
    "                                       , 'female_surprise':'female'\n",
    "                                       , 'female_neutral':'female'\n",
    "                                       , 'male_angry':'male'\n",
    "                                       , 'male_fear':'male'\n",
    "                                       , 'male_happy':'male'\n",
    "                                       , 'male_sad':'male'\n",
    "                                       , 'male_surprise':'male'\n",
    "                                       , 'male_neutral':'male'\n",
    "                                       , 'male_disgust':'male'\n",
    "                                      })\n",
    "\n",
    "modidf['predictedvalues'] = finaldf.predictedvalues.replace({'female_angry':'female'\n",
    "                                       , 'female_disgust':'female'\n",
    "                                       , 'female_fear':'female'\n",
    "                                       , 'female_happy':'female'\n",
    "                                       , 'female_sad':'female'\n",
    "                                       , 'female_surprise':'female'\n",
    "                                       , 'female_neutral':'female'\n",
    "                                       , 'male_angry':'male'\n",
    "                                       , 'male_fear':'male'\n",
    "                                       , 'male_happy':'male'\n",
    "                                       , 'male_sad':'male'\n",
    "                                       , 'male_surprise':'male'\n",
    "                                       , 'male_neutral':'male'\n",
    "                                       , 'male_disgust':'male'\n",
    "                                      })\n",
    "\n",
    "classes = modidf.actualvalues.unique()  \n",
    "classes.sort() \n",
    "\n",
    "# Confusion matrix \n",
    "c = confusion_matrix(modidf.actualvalues, modidf.predictedvalues)\n",
    "print(accuracy_score(modidf.actualvalues, modidf.predictedvalues))\n",
    "print_confusion_matrix(c, class_names = classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report \n",
    "classes = modidf.actualvalues.unique()\n",
    "classes.sort()    \n",
    "print(classification_report(modidf.actualvalues, modidf.predictedvalues, target_names=classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With just gender we get a 80% accuracy. The model is especially precise in capturing female voices. However, male voices tends to be harder and it does make higher mistakes thinking its female. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Emotion accuracy\n",
    "We'll now ignore the gender part and just super group them into the 7 core emotions. Lets see what we get..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modidf = pd.read_csv(\"Predictions.csv\")\n",
    "modidf['actualvalues'] = modidf.actualvalues.replace({'female_angry':'angry'\n",
    "                                       , 'female_disgust':'disgust'\n",
    "                                       , 'female_fear':'fear'\n",
    "                                       , 'female_happy':'happy'\n",
    "                                       , 'female_sad':'sad'\n",
    "                                       , 'female_surprise':'surprise'\n",
    "                                       , 'female_neutral':'neutral'\n",
    "                                       , 'male_angry':'angry'\n",
    "                                       , 'male_fear':'fear'\n",
    "                                       , 'male_happy':'happy'\n",
    "                                       , 'male_sad':'sad'\n",
    "                                       , 'male_surprise':'surprise'\n",
    "                                       , 'male_neutral':'neutral'\n",
    "                                       , 'male_disgust':'disgust'\n",
    "                                      })\n",
    "\n",
    "modidf['predictedvalues'] = modidf.predictedvalues.replace({'female_angry':'angry'\n",
    "                                       , 'female_disgust':'disgust'\n",
    "                                       , 'female_fear':'fear'\n",
    "                                       , 'female_happy':'happy'\n",
    "                                       , 'female_sad':'sad'\n",
    "                                       , 'female_surprise':'surprise'\n",
    "                                       , 'female_neutral':'neutral'\n",
    "                                       , 'male_angry':'angry'\n",
    "                                       , 'male_fear':'fear'\n",
    "                                       , 'male_happy':'happy'\n",
    "                                       , 'male_sad':'sad'\n",
    "                                       , 'male_surprise':'surprise'\n",
    "                                       , 'male_neutral':'neutral'\n",
    "                                       , 'male_disgust':'disgust'\n",
    "                                      })\n",
    "\n",
    "classes = modidf.actualvalues.unique() \n",
    "classes.sort() \n",
    "\n",
    "# Confusion matrix \n",
    "c = confusion_matrix(modidf.actualvalues, modidf.predictedvalues)\n",
    "print(accuracy_score(modidf.actualvalues, modidf.predictedvalues))\n",
    "print_confusion_matrix(c, class_names = classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report \n",
    "classes = modidf.actualvalues.unique()\n",
    "classes.sort()    \n",
    "print(classification_report(modidf.actualvalues, modidf.predictedvalues, target_names=classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "50%, not too shabby indeed. The precision and recall for 'Surprise' and 'Angry' is pretty good in particular "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"final\"></a>\n",
    "## 5. Final thoughts \n",
    "The gender seperation turns out to be a curcial implementation in order to accurately classify emotions. Upon closer inspection of the confusion matrix, it seems that female tends to express emotions in a more, obvious manner, for the lack of a better word. Whilst males tend to be very placid or subtle. This is probably why we see the error rate amongst males are really high. For example, male happy and angry gets mixed up quite often. \n",
    "\n",
    "In our next section we will be checking for generalisability of this initial baseline solution before  before implementing further enhancements, followed by an audio streamer that will give us the capability of predicting the emotions of a segment of the audio call.  \n",
    "\n",
    "This section of the notebook borrowed heavily from this [repository](https://github.com/MITESHPUTHRANNEU/Speech-Emotion-Analyzer). The original author may have overstated the accuracy as I wasn't able to replicate the accuracy results but, by in large the approach is pretty sound and I've taken his work as a blueprint to setup my own here. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
